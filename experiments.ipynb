{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qr requirements.txt # install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7mGmQbAO5pQb"
   },
   "source": [
    "# Setup\n",
    "\n",
    "Clone GitHub [repository](https://github.com/ultralytics/yolov5), install [dependencies](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) and check PyTorch and GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wbvMlHd_QwMG",
    "outputId": "e8225db4-e61d-4640-8b1f-8bfce3331cea"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import utils\n",
    "display = utils.notebook_init()  # checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4JnkELT0cIJg"
   },
   "source": [
    "# 1. Detect\n",
    "\n",
    "`detect.py` runs YOLOv5 inference on a variety of sources, downloading models automatically from the [latest YOLOv5 release](https://github.com/ultralytics/yolov5/releases), and saving results to `runs/detect`. Example inference sources are:\n",
    "\n",
    "```shell\n",
    "python detect.py --source 0  # webcam\n",
    "                          img.jpg  # image\n",
    "                          vid.mp4  # video\n",
    "                          screen  # screenshot\n",
    "                          path/  # directory\n",
    "                         'path/*.jpg'  # glob\n",
    "                         'https://youtu.be/LNwODJXcvt4'  # YouTube\n",
    "                         'rtsp://example.com/media.mp4'  # RTSP, RTMP, HTTP stream\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zR9ZbuQCH7FX",
    "outputId": "284ef04b-1596-412f-88f6-948828dd2b49"
   },
   "outputs": [],
   "source": [
    "!python detect.py --weights yolov5s.pt --img 640 --conf 0.25 --source data/images\n",
    "# display.Image(filename='runs/detect/exp/zidane.jpg', width=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hkAzDWJ7cWTr"
   },
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "<img align=\"left\" src=\"https://user-images.githubusercontent.com/26833433/127574988-6a558aa1-d268-44b9-bf6b-62d4c605cc72.jpg\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0eq1SMWl6Sfn"
   },
   "source": [
    "# 2. Validate\n",
    "Validate a model's accuracy on the [COCO](https://cocodataset.org/#home) dataset's `val` or `test` splits. Models are downloaded automatically from the [latest YOLOv5 release](https://github.com/ultralytics/yolov5/releases). To show results by class use the `--verbose` flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WQPtK1QYVaD_",
    "outputId": "cf7d52f0-281c-4c96-a488-79f5908f8426"
   },
   "outputs": [],
   "source": [
    "# Download COCO val\n",
    "torch.hub.download_url_to_file('https://ultralytics.com/assets/coco2017val.zip', 'tmp.zip')  # download (780M - 5000 images)\n",
    "!unzip -q tmp.zip -d ../datasets && rm tmp.zip  # unzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X58w8JLpMnjH",
    "outputId": "3e234e05-ee8b-4ad1-b1a4-f6a55d5e4f3d"
   },
   "outputs": [],
   "source": [
    "# Validate YOLOv5s on COCO val\n",
    "!python val.py --weights yolov5s.pt --data coco.yaml --img 640 --half"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1NcFxRcFdJ_O",
    "outputId": "bbeeea2b-04fc-4185-aa64-258690495b5a"
   },
   "outputs": [],
   "source": [
    "# Train YOLOv5s on COCO128 for 3 epochs\n",
    "!python train.py --img 640 --batch 16 --epochs 3 --data coco128.yaml --weights yolov5s.pt --cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5s.pt, cfg=models/yolov5s.yaml, data=coco128.yaml, hyp=data/hyps/hyp.scratch-low.yaml, epochs=3, classes=80, batch_size=16, imgsz=416, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=None, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs/train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
      "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 ‚úÖ\n",
      "YOLOv5 üöÄ v7.0-248-g6c91cbd Python-3.11.5 torch-2.1.1+cu121 CUDA:0 (NVIDIA GeForce RTX 3080, 12044MiB)\n",
      "\n",
      "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
      "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 üöÄ runs in Comet\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
      "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
      "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
      "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
      "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
      "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
      "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
      "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
      "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
      " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
      " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
      " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
      " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
      " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
      " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
      " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
      " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
      " 24      [17, 20, 23]  1    229245  models.yolo.Detect                      [80, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
      "YOLOv5s summary: 214 layers, 7235389 parameters, 7235389 gradients, 16.6 GFLOPs\n",
      "\n",
      "Transferred 348/349 items from yolov5s.pt\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ‚úÖ\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.0005), 60 bias\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/omar/datasets/coco128/labels/train2017.cache... 126 images\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/omar/datasets/coco128/labels/train2017.cache... 126 images, \u001b[0m\n",
      "\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m4.18 anchors/target, 0.977 Best Possible Recall (BPR). Anchors are a poor fit to dataset ‚ö†Ô∏è, attempting to improve...\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mWARNING ‚ö†Ô∏è Extremely small objects found: 16 of 929 labels are <3 pixels in size\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mRunning kmeans for 9 anchors on 927 points...\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mEvolving anchors with Genetic Algorithm: fitness = 0.6703: 100%|‚ñà‚ñà‚ñà‚ñà\u001b[0m\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mthr=0.25: 0.9946 best possible recall, 3.75 anchors past thr\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mn=9, img_size=416, metric_all=0.262/0.670-mean/best, past_thr=0.476-mean: 7,8, 14,16, 33,37, 88,58, 60,105, 91,196, 170,152, 231,270, 350,229\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mDone ‚úÖ (optional: update model *.yaml to use these anchors in the future)\n",
      "Plotting labels to runs/train/exp162/labels.jpg... \n",
      "Image sizes 416 train, 416 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns/train/exp162\u001b[0m\n",
      "Starting training for 3 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        0/2      1.54G    0.08073    0.05287    0.02397        194        416: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all        128        929      0.713      0.592      0.682      0.461\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        1/2      1.69G    0.06125    0.05308    0.02299        193        416: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all        128        929      0.713      0.592      0.682      0.461\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        2/2      1.69G    0.05705    0.06163    0.02465        249        416: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all        128        929      0.713      0.592      0.682      0.461\n",
      "\n",
      "3 epochs completed in 0.002 hours.\n",
      "Optimizer stripped from runs/train/exp162/weights/last.pt, 14.6MB\n",
      "Optimizer stripped from runs/train/exp162/weights/best.pt, 14.6MB\n",
      "\n",
      "Validating runs/train/exp162/weights/best.pt...\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
      "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
      "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
      "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
      "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
      "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
      "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
      "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
      "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
      " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
      " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
      " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
      " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
      " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
      " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
      " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
      " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
      " 24      [17, 20, 23]  1    229245  models.yolo.Detect                      [80, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLOv5s summary: 214 layers, 7235389 parameters, 7235389 gradients, 16.6 GFLOPs\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 157 layers, 7225885 parameters, 7225885 gradients, 16.4 GFLOPs\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all        128        929      0.531      0.587      0.604      0.379\n",
      "                     0        128        254      0.559      0.589      0.528      0.305\n",
      "                     1        128          6      0.617      0.283      0.293      0.239\n",
      "                     2        128         46      0.556      0.191      0.183     0.0981\n",
      "                     3        128          5      0.769          1      0.995      0.582\n",
      "                     4        128          6      0.761      0.833      0.955      0.652\n",
      "                     5        128          7       0.54      0.714      0.729      0.585\n",
      "                     6        128          3      0.754      0.667      0.863      0.588\n",
      "                     7        128         12      0.534      0.167      0.334      0.183\n",
      "                     8        128          6      0.355      0.333      0.485      0.285\n",
      "                     9        128         14      0.561      0.143        0.2      0.124\n",
      "                    11        128          2      0.674          1      0.995      0.697\n",
      "                    13        128          9      0.474      0.333      0.468      0.236\n",
      "                    14        128         16      0.684          1      0.929      0.552\n",
      "                    15        128          4      0.416          1      0.912      0.627\n",
      "                    16        128          9      0.716      0.778      0.837      0.611\n",
      "                    17        128          2      0.453          1      0.995      0.497\n",
      "                    20        128         17      0.564      0.941      0.852      0.569\n",
      "                    21        128          1      0.403          1      0.995      0.995\n",
      "                    22        128          4      0.882          1      0.995      0.871\n",
      "                    23        128          9      0.695      0.889      0.853      0.585\n",
      "                    24        128          6      0.542      0.333      0.468      0.211\n",
      "                    25        128         18      0.611      0.699      0.654       0.32\n",
      "                    26        128         19          1     0.0973      0.267      0.136\n",
      "                    27        128          7      0.495      0.429      0.469      0.246\n",
      "                    28        128          4      0.491          1      0.674      0.453\n",
      "                    29        128          5       0.39        0.6      0.671      0.522\n",
      "                    30        128          1      0.692          1      0.995      0.163\n",
      "                    31        128          7      0.611      0.714      0.785      0.366\n",
      "                    32        128          6      0.317      0.167      0.254      0.163\n",
      "                    33        128         10      0.419        0.3      0.261     0.0804\n",
      "                    34        128          4      0.272       0.25      0.239      0.076\n",
      "                    35        128          7      0.447      0.286      0.257      0.113\n",
      "                    36        128          5      0.444        0.4      0.548      0.258\n",
      "                    38        128          7       0.47      0.429      0.453      0.224\n",
      "                    39        128         18      0.366      0.258      0.235      0.104\n",
      "                    40        128         16      0.513      0.375      0.506      0.243\n",
      "                    41        128         36      0.633      0.431      0.513       0.29\n",
      "                    42        128          6       0.39      0.167      0.303      0.204\n",
      "                    43        128         16      0.263      0.375      0.335       0.16\n",
      "                    44        128         22      0.478      0.273      0.377      0.178\n",
      "                    45        128         28      0.523      0.679      0.515      0.346\n",
      "                    46        128          1      0.457          1      0.995      0.112\n",
      "                    48        128          2      0.219      0.329      0.497      0.361\n",
      "                    49        128          4      0.715      0.644      0.895      0.549\n",
      "                    50        128         11       0.42      0.182      0.249      0.193\n",
      "                    51        128         24       0.76      0.527       0.59      0.309\n",
      "                    52        128          2      0.376          1      0.828      0.745\n",
      "                    53        128          5       0.52        0.8      0.866      0.617\n",
      "                    54        128         14      0.352          1      0.748      0.563\n",
      "                    55        128          4      0.441          1      0.995      0.827\n",
      "                    56        128         35      0.423      0.514      0.382      0.198\n",
      "                    57        128          6      0.469        0.5       0.58      0.448\n",
      "                    58        128         14      0.647      0.571      0.565      0.294\n",
      "                    59        128          3      0.663      0.659       0.83      0.588\n",
      "                    60        128         13      0.521      0.503      0.449       0.34\n",
      "                    61        128          2      0.579          1      0.828      0.713\n",
      "                    62        128          2      0.432          1      0.828      0.679\n",
      "                    63        128          3      0.556      0.445      0.626      0.318\n",
      "                    64        128          2          1          0      0.258      0.129\n",
      "                    65        128          8       0.49        0.5      0.454      0.246\n",
      "                    67        128          8      0.322       0.25      0.302     0.0891\n",
      "                    68        128          3      0.464          1      0.863      0.594\n",
      "                    69        128          5      0.411        0.6      0.418      0.285\n",
      "                    71        128          6      0.209      0.167       0.28      0.197\n",
      "                    72        128          5      0.514        0.8      0.765      0.547\n",
      "                    73        128         29      0.289     0.0345      0.206       0.07\n",
      "                    74        128          9      0.588      0.889      0.804      0.546\n",
      "                    75        128          2      0.308          1      0.695      0.586\n",
      "                    76        128          1          1          0      0.332     0.0616\n",
      "                    77        128         21      0.584      0.667      0.677      0.374\n",
      "                    79        128          5      0.653          1      0.928      0.579\n",
      "Results saved to \u001b[1mruns/train/exp162\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python train.py --data coco128.yaml --weights 'yolov5s.pt' --cfg models/yolov5s.yaml --img 416 --epochs 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IEijrePND_2I"
   },
   "source": [
    "# Train QuantModel, not pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mweights=, cfg=models/yolov5s_quant.yaml, data=coco128.yaml, hyp=data/hyps/hyp.scratch-low.yaml, epochs=1, classes=80, batch_size=16, imgsz=416, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=None, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs/train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
      "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 ‚úÖ\n",
      "YOLOv5 üöÄ v7.0-248-g6c91cbd Python-3.11.5 torch-2.1.1+cu121 CUDA:0 (NVIDIA GeForce RTX 3080, 12044MiB)\n",
      "\n",
      "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
      "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 üöÄ runs in Comet\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
      "Overriding model.yaml nc=3 with nc=80\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      3521  models.common.QuantConv                 [3, 32, 6, 2, 2]              \n",
      "  1                -1  1     18561  models.common.QuantConv                 [32, 64, 3, 2]                \n",
      "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
      "  3                -1  1     73985  models.common.QuantConv                 [64, 128, 3, 2]               \n",
      "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
      "  5                -1  1    295425  models.common.QuantConv                 [128, 256, 3, 2]              \n",
      "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
      "  7                -1  1   1180673  models.common.QuantConv                 [256, 512, 3, 2]              \n",
      "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
      "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
      " 10                -1  1    131585  models.common.QuantConv                 [512, 256, 1, 1]              \n",
      " 11                -1  1         0  brevitas.nn.quant_upsample.QuantUpsample[None, 2, 'nearest']          \n",
      " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
      " 14                -1  1     33025  models.common.QuantConv                 [256, 128, 1, 1]              \n",
      " 15                -1  1         0  brevitas.nn.quant_upsample.QuantUpsample[None, 2, 'nearest']          \n",
      " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
      " 18                -1  1    147713  models.common.QuantConv                 [128, 128, 3, 2]              \n",
      " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
      " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
      " 21                -1  1    590337  models.common.QuantConv                 [256, 256, 3, 2]              \n",
      " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
      " 24      [17, 20, 23]  1    229245  models.yolo.Detect                      [80, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
      "YOLOv5s_quant summary: 800 layers, 7235390 parameters, 7235390 gradients, 10.9 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ‚úÖ\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 48 weight(decay=0.0), 88 weight(decay=0.0005), 60 bias\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/omar/datasets/coco128/labels/train2017.cache... 126 images\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/omar/datasets/coco128/labels/train2017.cache... 126 images, \u001b[0m\n",
      "\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m4.18 anchors/target, 0.977 Best Possible Recall (BPR). Anchors are a poor fit to dataset ‚ö†Ô∏è, attempting to improve...\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mWARNING ‚ö†Ô∏è Extremely small objects found: 16 of 929 labels are <3 pixels in size\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mRunning kmeans for 9 anchors on 927 points...\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mEvolving anchors with Genetic Algorithm: fitness = 0.6703: 100%|‚ñà‚ñà‚ñà‚ñà\u001b[0m\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mthr=0.25: 0.9946 best possible recall, 3.75 anchors past thr\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mn=9, img_size=416, metric_all=0.262/0.670-mean/best, past_thr=0.476-mean: 7,8, 14,16, 33,37, 88,58, 60,105, 91,196, 170,152, 231,270, 350,229\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mDone ‚úÖ (optional: update model *.yaml to use these anchors in the future)\n",
      "Plotting labels to runs/train/exp163/labels.jpg... \n",
      "Image sizes 416 train, 416 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns/train/exp163\u001b[0m\n",
      "Starting training for 1 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        0/0      2.02G     0.1088    0.05391     0.1046        194        416: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all        128        929          0          0          0          0\n",
      "\n",
      "1 epochs completed in 0.001 hours.\n",
      "Optimizer stripped from runs/train/exp163/weights/last.pt, 14.7MB\n",
      "Optimizer stripped from runs/train/exp163/weights/best.pt, 14.7MB\n",
      "\n",
      "Validating runs/train/exp163/weights/best.pt...\n",
      "Overriding model.yaml nc=3 with nc=80\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      3521  models.common.QuantConv                 [3, 32, 6, 2, 2]              \n",
      "  1                -1  1     18561  models.common.QuantConv                 [32, 64, 3, 2]                \n",
      "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
      "  3                -1  1     73985  models.common.QuantConv                 [64, 128, 3, 2]               \n",
      "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
      "  5                -1  1    295425  models.common.QuantConv                 [128, 256, 3, 2]              \n",
      "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
      "  7                -1  1   1180673  models.common.QuantConv                 [256, 512, 3, 2]              \n",
      "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
      "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
      " 10                -1  1    131585  models.common.QuantConv                 [512, 256, 1, 1]              \n",
      " 11                -1  1         0  brevitas.nn.quant_upsample.QuantUpsample[None, 2, 'nearest']          \n",
      " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
      " 14                -1  1     33025  models.common.QuantConv                 [256, 128, 1, 1]              \n",
      " 15                -1  1         0  brevitas.nn.quant_upsample.QuantUpsample[None, 2, 'nearest']          \n",
      " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
      " 18                -1  1    147713  models.common.QuantConv                 [128, 128, 3, 2]              \n",
      " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
      " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
      " 21                -1  1    590337  models.common.QuantConv                 [256, 256, 3, 2]              \n",
      " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
      " 24      [17, 20, 23]  1    229245  models.yolo.Detect                      [80, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
      "YOLOv5s_quant summary: 800 layers, 7235390 parameters, 7235389 gradients, 10.9 GFLOPs\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s_quant summary: 752 layers, 7227646 parameters, 7227646 gradients, 10.8 GFLOPs\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all        128        929          0          0          0          0\n",
      "Results saved to \u001b[1mruns/train/exp163\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python train.py --data coco128.yaml --weights '' --cfg models/yolov5s_quant.yaml --img 416 --epochs 1 --classes 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = {}\n",
    "ckpt['model'] = model.state_dict()\n",
    "torch.save(ckpt,\"experiment_models/yolov5s.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Quantized (QAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mweights=experiment_models/yolov5s_quant.pt, cfg=models/yolov5s_quant.yaml, data=coco128.yaml, hyp=data/hyps/hyp.scratch-low.yaml, epochs=3, classes=80, batch_size=16, imgsz=416, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=None, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs/train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
      "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 ‚úÖ\n",
      "YOLOv5 üöÄ v7.0-248-g6c91cbd Python-3.11.5 torch-2.1.1+cu121 CUDA:0 (NVIDIA GeForce RTX 3080, 12044MiB)\n",
      "\n",
      "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
      "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 üöÄ runs in Comet\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
      "Overriding model.yaml nc=3 with nc=80\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      3521  models.common.QuantConv                 [3, 32, 6, 2, 2]              \n",
      "  1                -1  1     18561  models.common.QuantConv                 [32, 64, 3, 2]                \n",
      "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
      "  3                -1  1     73985  models.common.QuantConv                 [64, 128, 3, 2]               \n",
      "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
      "  5                -1  1    295425  models.common.QuantConv                 [128, 256, 3, 2]              \n",
      "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
      "  7                -1  1   1180673  models.common.QuantConv                 [256, 512, 3, 2]              \n",
      "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
      "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
      " 10                -1  1    131585  models.common.QuantConv                 [512, 256, 1, 1]              \n",
      " 11                -1  1         0  brevitas.nn.quant_upsample.QuantUpsample[None, 2, 'nearest']          \n",
      " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
      " 14                -1  1     33025  models.common.QuantConv                 [256, 128, 1, 1]              \n",
      " 15                -1  1         0  brevitas.nn.quant_upsample.QuantUpsample[None, 2, 'nearest']          \n",
      " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
      " 18                -1  1    147713  models.common.QuantConv                 [128, 128, 3, 2]              \n",
      " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
      " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
      " 21                -1  1    590337  models.common.QuantConv                 [256, 256, 3, 2]              \n",
      " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
      " 24      [17, 20, 23]  1    229245  models.yolo.Detect                      [80, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
      "YOLOv5s_quant summary: 800 layers, 7235390 parameters, 7235390 gradients, 10.9 GFLOPs\n",
      "\n",
      "Overriding model.yaml nc=3 with nc=80\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      3521  models.common.QuantConv                 [3, 32, 6, 2, 2]              \n",
      "  1                -1  1     18561  models.common.QuantConv                 [32, 64, 3, 2]                \n",
      "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
      "  3                -1  1     73985  models.common.QuantConv                 [64, 128, 3, 2]               \n",
      "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
      "  5                -1  1    295425  models.common.QuantConv                 [128, 256, 3, 2]              \n",
      "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
      "  7                -1  1   1180673  models.common.QuantConv                 [256, 512, 3, 2]              \n",
      "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
      "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
      " 10                -1  1    131585  models.common.QuantConv                 [512, 256, 1, 1]              \n",
      " 11                -1  1         0  brevitas.nn.quant_upsample.QuantUpsample[None, 2, 'nearest']          \n",
      " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
      " 14                -1  1     33025  models.common.QuantConv                 [256, 128, 1, 1]              \n",
      " 15                -1  1         0  brevitas.nn.quant_upsample.QuantUpsample[None, 2, 'nearest']          \n",
      " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
      " 18                -1  1    147713  models.common.QuantConv                 [128, 128, 3, 2]              \n",
      " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
      " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
      " 21                -1  1    590337  models.common.QuantConv                 [256, 256, 3, 2]              \n",
      " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
      " 24      [17, 20, 23]  1    229245  models.yolo.Detect                      [80, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
      "YOLOv5s_quant summary: 800 layers, 7235390 parameters, 7235390 gradients, 10.9 GFLOPs\n",
      "\n",
      "Transferred 330/331 items from experiment_models/yolov5s_quant.pt\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ‚úÖ\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 48 weight(decay=0.0), 88 weight(decay=0.0005), 60 bias\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/omar/datasets/coco128/labels/train2017.cache... 126 images\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/omar/datasets/coco128/labels/train2017.cache... 126 images, \u001b[0m\n",
      "\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m4.18 anchors/target, 0.977 Best Possible Recall (BPR). Anchors are a poor fit to dataset ‚ö†Ô∏è, attempting to improve...\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mWARNING ‚ö†Ô∏è Extremely small objects found: 16 of 929 labels are <3 pixels in size\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mRunning kmeans for 9 anchors on 927 points...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mEvolving anchors with Genetic Algorithm: fitness = 0.6703: 100%|‚ñà‚ñà‚ñà‚ñà\u001b[0m\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mthr=0.25: 0.9946 best possible recall, 3.75 anchors past thr\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mn=9, img_size=416, metric_all=0.262/0.670-mean/best, past_thr=0.476-mean: 7,8, 14,16, 33,37, 88,58, 60,105, 91,196, 170,152, 231,270, 350,229\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mDone ‚úÖ (optional: update model *.yaml to use these anchors in the future)\n",
      "Plotting labels to runs/train/exp164/labels.jpg... \n",
      "Image sizes 416 train, 416 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns/train/exp164\u001b[0m\n",
      "Starting training for 3 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        0/2      2.04G    0.09838    0.06316     0.1022        194        416: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all        128        929          0          0          0          0\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        1/2       2.2G    0.09851    0.05957     0.1016        193        416: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all        128        929          0          0          0          0\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        2/2       2.2G    0.09892    0.06868     0.1023        249        416: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all        128        929          0          0          0          0\n",
      "\n",
      "3 epochs completed in 0.002 hours.\n",
      "Optimizer stripped from runs/train/exp164/weights/last.pt, 14.7MB\n",
      "Optimizer stripped from runs/train/exp164/weights/best.pt, 14.7MB\n",
      "\n",
      "Validating runs/train/exp164/weights/best.pt...\n",
      "Overriding model.yaml nc=3 with nc=80\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      3521  models.common.QuantConv                 [3, 32, 6, 2, 2]              \n",
      "  1                -1  1     18561  models.common.QuantConv                 [32, 64, 3, 2]                \n",
      "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
      "  3                -1  1     73985  models.common.QuantConv                 [64, 128, 3, 2]               \n",
      "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
      "  5                -1  1    295425  models.common.QuantConv                 [128, 256, 3, 2]              \n",
      "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
      "  7                -1  1   1180673  models.common.QuantConv                 [256, 512, 3, 2]              \n",
      "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
      "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
      " 10                -1  1    131585  models.common.QuantConv                 [512, 256, 1, 1]              \n",
      " 11                -1  1         0  brevitas.nn.quant_upsample.QuantUpsample[None, 2, 'nearest']          \n",
      " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
      " 14                -1  1     33025  models.common.QuantConv                 [256, 128, 1, 1]              \n",
      " 15                -1  1         0  brevitas.nn.quant_upsample.QuantUpsample[None, 2, 'nearest']          \n",
      " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
      " 18                -1  1    147713  models.common.QuantConv                 [128, 128, 3, 2]              \n",
      " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
      " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
      " 21                -1  1    590337  models.common.QuantConv                 [256, 256, 3, 2]              \n",
      " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
      " 24      [17, 20, 23]  1    229245  models.yolo.Detect                      [80, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
      "YOLOv5s_quant summary: 800 layers, 7235390 parameters, 7235389 gradients, 10.9 GFLOPs\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s_quant summary: 752 layers, 7227646 parameters, 7227646 gradients, 10.8 GFLOPs\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all        128        929          0          0          0          0\n",
      "Results saved to \u001b[1mruns/train/exp164\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# please do not forget to add the --cfg for quant models so the code does not break\n",
    "!python train.py --img 416 --batch 16 --epochs 3 --data coco128.yaml --cfg models/yolov5s_quant.yaml --weights experiment_models/yolov5s_quant.pt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train PostQuantization (PTQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# please do not forget to add the --cfg for quant models so the code does not break\n",
    "!python train.py --img 416 --batch 16 --epochs 9 --data coco128.yaml --cfg models/yolov5s_quant.yaml --weights yolov5s.pt --device 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 üöÄ v7.0-248-g6c91cbd Python-3.10.12 torch-2.1.1+cu121 CUDA:0 (NVIDIA GeForce RTX 3080, 12044MiB)\n",
      "\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
      "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
      "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
      "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
      "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
      "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
      "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
      "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
      "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
      " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
      " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
      " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
      " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
      " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
      " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
      " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
      " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
      " 24      [17, 20, 23]  1    229245  models.yolo.Detect                      [80, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
      "YOLOv5s summary: 214 layers, 7235389 parameters, 7235389 gradients, 16.6 GFLOPs\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 157 layers, 7225885 parameters, 7225885 gradients, 16.4 GFLOPs\n",
      "Adding AutoShape... \n",
      "image 1/1: 720x1280 3 0s, 2 27s\n",
      "Speed: 1193.2ms pre-process, 14.6ms inference, 1.2ms NMS per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "source": [
    "# YOLOv5 PyTorch HUB Inference (DetectionModels only)\n",
    "import torch\n",
    "\n",
    "model_pre = torch.hub.load('.',\n",
    "                       'custom',\n",
    "                       'experiment_models/yolov5s.pt',\n",
    "                       source='local',\n",
    "                       classes = 80,\n",
    "                       cfg = \"models/yolov5s.yaml\",\n",
    "                       force_reload=True\n",
    "                      )\n",
    "\n",
    "im = 'https://ultralytics.com/images/zidane.jpg'  # file, Path, PIL.Image, OpenCV, nparray, list\n",
    "results = model_pre(im)  # inference\n",
    "results.print()  # or .show(), .save(), .crop(), .pandas(), etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key,value in model_pre.state_dict().items():\n",
    "    print(key,value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Quantized Model (QAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GMusP4OAxFu6"
   },
   "outputs": [],
   "source": [
    "# YOLOv5 PyTorch HUB Inference (DetectionModels only)\n",
    "import torch\n",
    "\n",
    "model = torch.hub.load('.',\n",
    "                       'custom',\n",
    "                       'experiment_models/yolov5s_quant_pre.pt',\n",
    "                       source='local',\n",
    "                       classes = 80,\n",
    "                       cfg = \"models/yolov5s_quant.yaml\",\n",
    "                       force_reload=True\n",
    "                      )\n",
    "\n",
    "im = 'https://ultralytics.com/images/zidane.jpg'  # file, Path, PIL.Image, OpenCV, nparray, list\n",
    "results = model(im)  # inference\n",
    "results.print()  # or .show(), .save(), .crop(), .pandas(), etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model(im)  # inference\n",
    "results.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key,value in model.state_dict().items():\n",
    "    print(key,value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(model_pre.state_dict(), strict = False)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Quantized Model (PTQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOLOv5 PyTorch HUB Inference (DetectionModels only)\n",
    "import torch\n",
    "\n",
    "model = torch.hub.load('.',\n",
    "                       'custom',\n",
    "                       'experiment_models/yolov5s_quant_pre.pt',\n",
    "                       source='local',\n",
    "                       classes = 80,\n",
    "                       force_reload=True\n",
    "                      )\n",
    "\n",
    "im = 'https://ultralytics.com/images/zidane.jpg'  # file, Path, PIL.Image, OpenCV, nparray, list\n",
    "results = model(im)  # inference\n",
    "results.print()  # or .show(), .save(), .crop(), .pandas(), etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = results.pandas()\n",
    "df.xyxy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Copying weights for PTQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 üöÄ v7.0-248-g6c91cbd Python-3.10.12 torch-2.1.1+cu121 CUDA:0 (NVIDIA GeForce RTX 3080, 12044MiB)\n",
      "\n",
      "Overriding model.yaml nc=3 with nc=80\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      3521  models.common.QuantConv                 [3, 32, 6, 2, 2]              \n",
      "  1                -1  1     18561  models.common.QuantConv                 [32, 64, 3, 2]                \n",
      "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
      "  3                -1  1     73985  models.common.QuantConv                 [64, 128, 3, 2]               \n",
      "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
      "  5                -1  1    295425  models.common.QuantConv                 [128, 256, 3, 2]              \n",
      "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
      "  7                -1  1   1180673  models.common.QuantConv                 [256, 512, 3, 2]              \n",
      "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
      "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
      " 10                -1  1    131585  models.common.QuantConv                 [512, 256, 1, 1]              \n",
      " 11                -1  1         0  brevitas.nn.quant_upsample.QuantUpsample[None, 2, 'nearest']          \n",
      " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
      " 14                -1  1     33025  models.common.QuantConv                 [256, 128, 1, 1]              \n",
      " 15                -1  1         0  brevitas.nn.quant_upsample.QuantUpsample[None, 2, 'nearest']          \n",
      " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
      " 18                -1  1    147713  models.common.QuantConv                 [128, 128, 3, 2]              \n",
      " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
      " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
      " 21                -1  1    590337  models.common.QuantConv                 [256, 256, 3, 2]              \n",
      " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
      " 24      [17, 20, 23]  1    229245  models.yolo.Detect                      [80, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
      "YOLOv5s_quant summary: 800 layers, 7235390 parameters, 7235390 gradients, 10.9 GFLOPs\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s_quant summary: 752 layers, 7227646 parameters, 7227646 gradients, 10.8 GFLOPs\n",
      "Adding AutoShape... \n",
      "YOLOv5 üöÄ v7.0-248-g6c91cbd Python-3.10.12 torch-2.1.1+cu121 CUDA:0 (NVIDIA GeForce RTX 3080, 12044MiB)\n",
      "\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
      "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
      "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
      "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
      "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
      "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
      "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
      "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
      "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
      " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
      " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
      " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
      " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
      " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
      " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
      " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
      " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
      " 24      [17, 20, 23]  1    229245  models.yolo.Detect                      [80, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
      "YOLOv5s summary: 214 layers, 7235389 parameters, 7235389 gradients, 16.6 GFLOPs\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 157 layers, 7225885 parameters, 7225885 gradients, 16.4 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "# YOLOv5 PyTorch HUB Inference (DetectionModels only)\n",
    "import torch\n",
    "\n",
    "model_quant = torch.hub.load('.',\n",
    "                       'custom',\n",
    "                       'experiment_models/yolov5s_quant.pt',\n",
    "                       source='local',\n",
    "                       classes = 80,\n",
    "                       cfg = \"models/yolov5s_quant.yaml\",\n",
    "                       force_reload=False\n",
    "                      )\n",
    "\n",
    "model = torch.hub.load('.',\n",
    "                       'custom',\n",
    "                       'experiment_models/yolov5s.pt',\n",
    "                       source='local',\n",
    "                       classes = 80,\n",
    "                       cfg = \"models/yolov5s.yaml\",\n",
    "                       force_reload=False\n",
    "                      )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 3.82984e-03,  1.28975e-01,  1.35297e-01,  2.92381e-01,  1.65547e-01,  1.44634e-01],\n",
       "          [ 9.60014e-02,  2.49973e-01,  2.84989e-01,  3.86146e-01,  2.49779e-01,  1.71480e-01],\n",
       "          [ 6.13261e-02,  2.40636e-01,  3.78559e-01,  4.74268e-01,  2.02508e-01,  7.06150e-02],\n",
       "          [ 7.99039e-02,  1.92197e-01,  2.95299e-01,  3.52881e-01,  5.88459e-02, -4.01465e-02],\n",
       "          [-1.49595e-01, -7.84449e-02, -4.27240e-02, -1.40306e-02, -2.46472e-01, -3.21756e-01],\n",
       "          [-2.78570e-01, -2.76819e-01, -3.36735e-01, -2.39858e-01, -3.64747e-01, -3.45099e-01]],\n",
       "\n",
       "         [[-2.56296e-02,  1.49546e-02, -2.90339e-02,  1.05631e-01,  4.50584e-02,  1.15746e-01],\n",
       "          [ 9.69255e-02,  1.86264e-01,  1.61072e-01,  2.30520e-01,  1.24209e-01,  1.29947e-01],\n",
       "          [ 1.13315e-01,  2.28575e-01,  3.01135e-01,  3.47628e-01,  9.11381e-02,  3.52589e-02],\n",
       "          [ 1.34908e-01,  1.69924e-01,  2.02508e-01,  2.22350e-01, -7.81045e-02, -8.67125e-02],\n",
       "          [-8.31137e-02, -7.97094e-02, -1.00865e-01, -1.02810e-01, -3.53853e-01, -3.67471e-01],\n",
       "          [-9.28403e-02, -1.44148e-01, -2.35383e-01, -1.37728e-01, -3.03859e-01, -2.59894e-01]],\n",
       "\n",
       "         [[ 2.89852e-02,  9.96975e-02,  7.65483e-02,  1.91030e-01,  1.09035e-01,  1.34713e-01],\n",
       "          [ 1.15552e-01,  2.18265e-01,  2.01535e-01,  2.48028e-01,  1.27418e-01,  1.13607e-01],\n",
       "          [ 9.89680e-02,  2.06787e-01,  2.86934e-01,  3.00163e-01,  6.59463e-02,  3.47422e-03],\n",
       "          [ 1.24792e-01,  1.38020e-01,  1.68367e-01,  1.79747e-01, -8.98250e-02, -7.90772e-02],\n",
       "          [-5.62197e-02, -7.05178e-02, -7.86881e-02, -7.98067e-02, -2.69232e-01, -2.70010e-01],\n",
       "          [-1.35321e-02, -6.86211e-02, -1.36464e-01, -3.25598e-02, -1.70313e-01, -1.26640e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 1.57730e-02, -1.32307e-01, -9.66445e-02, -5.94099e-02,  1.02271e-01,  7.30212e-02],\n",
       "          [ 4.11070e-01,  1.57957e-01,  1.63253e-01, -6.92398e-01, -5.65139e-02,  1.14269e-01],\n",
       "          [-1.93951e-01, -1.55161e+00,  5.71262e-01,  2.29531e-01, -4.34238e-01, -5.56699e-01],\n",
       "          [-2.77754e+00, -3.50303e+00,  1.93289e+00,  4.15704e+00,  1.81639e+00, -9.16799e-02],\n",
       "          [-1.55690e+00, -2.50349e+00,  8.29422e-01,  3.30180e+00,  1.21799e+00,  9.50724e-02],\n",
       "          [ 7.28143e-01, -4.97785e-01,  2.73219e-01, -1.68631e-01, -7.87057e-01, -3.92866e-01]],\n",
       "\n",
       "         [[ 4.53435e-01, -1.49518e-01,  8.99423e-02, -2.39625e-01, -3.04000e-01, -3.88233e-01],\n",
       "          [ 5.06060e-01, -1.73100e-01, -2.25269e-02, -9.39967e-01, -4.32252e-01, -2.82321e-01],\n",
       "          [-3.08468e-01, -1.86405e+00,  1.00616e+00,  7.80437e-01, -2.22249e-01, -4.17028e-01],\n",
       "          [-3.77311e+00, -4.42446e+00,  2.23077e+00,  5.30618e+00,  2.52467e+00,  4.35893e-01],\n",
       "          [-2.72988e+00, -3.52157e+00,  8.65829e-01,  3.84460e+00,  1.55161e+00,  2.45417e-01],\n",
       "          [ 3.79297e-01, -8.96940e-01,  4.55752e-01,  2.51044e-01, -4.76272e-01, -3.10123e-01]],\n",
       "\n",
       "         [[ 1.84208e-02, -1.23453e-01, -1.73100e-01,  2.94898e-01,  2.57663e-01,  1.27094e-01],\n",
       "          [ 4.72962e-01,  1.96930e-01,  1.40002e-01, -4.83222e-01, -6.78497e-03,  2.25890e-01],\n",
       "          [ 8.71124e-01, -5.72255e-01,  8.90983e-01,  9.65618e-02, -5.76226e-01, -1.17248e-01],\n",
       "          [-9.55192e-01, -2.61205e+00,  1.07169e+00,  2.19767e+00,  1.08229e-01, -5.15658e-01],\n",
       "          [-3.84923e-01, -1.90112e+00, -4.06436e-01,  1.39671e+00, -7.75721e-06, -3.65065e-01],\n",
       "          [ 1.37553e+00,  1.77795e-02,  1.62343e-01, -2.42108e-01, -3.63079e-01,  8.31159e-02]]],\n",
       "\n",
       "\n",
       "        [[[ 4.86383e-01,  6.07766e-01,  9.77009e-01,  1.05680e+00,  8.70056e-01,  6.14557e-01],\n",
       "          [ 3.95133e-01,  5.76359e-01,  8.27614e-01,  8.41196e-01,  8.69207e-01,  5.60656e-01],\n",
       "          [ 5.91532e-02,  1.74436e-01,  1.83667e-01,  2.15790e-02,  2.32369e-01,  2.16984e-01],\n",
       "          [-1.39527e-01, -2.22819e-01, -2.27063e-01, -3.51206e-01, -2.96668e-01, -1.19686e-01],\n",
       "          [-2.55924e-01, -4.91900e-01, -8.41196e-01, -1.10688e+00, -9.71068e-01, -3.88979e-01],\n",
       "          [-3.14281e-01, -4.57522e-01, -8.85335e-01, -1.24015e+00, -1.01860e+00, -4.11048e-01]],\n",
       "\n",
       "         [[ 2.79903e-01,  3.74336e-01,  7.10900e-01,  8.15306e-01,  6.82888e-01,  3.93223e-01],\n",
       "          [ 2.93909e-01,  4.40121e-01,  6.71853e-01,  7.08353e-01,  8.00027e-01,  4.51580e-01],\n",
       "          [ 7.02411e-02,  1.68494e-01,  1.56822e-01,  2.26533e-02,  2.93909e-01,  2.22607e-01],\n",
       "          [-8.58915e-02, -1.92686e-01, -1.98734e-01, -2.89877e-01, -1.90245e-01, -7.89417e-02],\n",
       "          [-2.16241e-01, -4.43516e-01, -7.59283e-01, -9.81254e-01, -8.26341e-01, -3.15767e-01],\n",
       "          [-2.06055e-01, -3.20860e-01, -7.03260e-01, -1.01521e+00, -7.89417e-01, -2.70142e-01]],\n",
       "\n",
       "         [[ 1.18837e-01,  1.96930e-01,  4.81714e-01,  5.88667e-01,  4.42668e-01,  2.48284e-01],\n",
       "          [ 1.49713e-01,  2.66746e-01,  4.36301e-01,  4.82987e-01,  5.30522e-01,  2.81813e-01],\n",
       "          [ 5.09036e-02,  1.32206e-01,  1.06688e-01, -1.16052e-02,  1.85046e-01,  1.44939e-01],\n",
       "          [-3.22557e-02, -7.34242e-02, -3.25741e-02, -8.92338e-02, -8.82789e-02, -2.78259e-02],\n",
       "          [-9.60775e-02, -2.53802e-01, -4.62191e-01, -6.31958e-01, -5.79754e-01, -1.83242e-01],\n",
       "          [-6.63683e-02, -1.56716e-01, -4.69406e-01, -7.31272e-01, -5.65749e-01, -1.21702e-01]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[ 1.06954e-01,  5.51430e-02,  2.78374e-01,  3.24257e-01, -3.50211e-02,  1.10414e-01],\n",
       "          [ 5.74179e-02,  5.01125e-02,  4.46270e-01,  5.48546e-01,  1.00930e-02,  4.14133e-03],\n",
       "          [ 2.09806e-01,  3.21694e-01,  8.83313e-01,  1.04531e+00,  3.15286e-01,  1.34061e-01],\n",
       "          [ 3.43482e-01,  5.83920e-01,  1.19347e+00,  1.33804e+00,  5.81869e-01,  3.06571e-01],\n",
       "          [-1.45387e-02,  1.04134e-01,  6.02375e-01,  7.63863e-01,  2.82732e-01,  1.41110e-01],\n",
       "          [ 9.31759e-02,  1.06633e-01,  3.21438e-01,  4.27558e-01,  9.39449e-02,  1.27396e-01]],\n",
       "\n",
       "         [[ 1.58027e-01, -3.49250e-02, -2.64180e-02, -1.20475e-02, -1.49569e-01,  2.04551e-01],\n",
       "          [-8.27305e-02, -2.75298e-01, -5.74820e-02,  5.09135e-02, -2.59534e-01, -7.17724e-02],\n",
       "          [-1.37521e-01, -1.88274e-01,  2.51460e-01,  4.77799e-01, -7.72835e-02, -1.20411e-01],\n",
       "          [-4.87668e-02,  4.87347e-02,  5.70078e-01,  7.57199e-01,  1.29190e-01, -4.77735e-02],\n",
       "          [-3.04776e-01, -2.82476e-01,  1.21116e-01,  3.05289e-01, -1.96253e-02, -6.77352e-02],\n",
       "          [-1.02532e-01, -1.48671e-01, -3.59503e-02,  7.75398e-02, -8.80493e-02,  2.34862e-02]],\n",
       "\n",
       "         [[-2.44539e-01, -3.33998e-01, -2.89140e-01, -2.28646e-01, -3.92954e-01, -1.93785e-01],\n",
       "          [-2.49922e-01, -3.00675e-01,  6.67099e-02,  2.24930e-01, -1.65717e-01, -2.27621e-01],\n",
       "          [-1.55208e-01, -1.60046e-02,  6.25445e-01,  9.53035e-01,  2.59150e-01, -7.35667e-02],\n",
       "          [-6.39864e-02,  1.86736e-01,  8.67933e-01,  1.13913e+00,  3.50659e-01, -8.03595e-02],\n",
       "          [-3.77318e-01, -2.61585e-01,  2.21085e-01,  4.34992e-01, -1.22558e-02, -2.11472e-01],\n",
       "          [-2.84526e-01, -3.11441e-01, -2.09550e-01, -8.01672e-02, -3.05802e-01, -2.59021e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 3.12198e-01,  2.90126e-01,  2.45451e-01,  2.77362e-01,  9.42711e-02, -7.42601e-02],\n",
       "          [ 1.80032e-01,  1.26249e-01,  8.37670e-02,  1.60886e-01,  2.87035e-02, -8.86866e-02],\n",
       "          [ 1.12952e-01,  4.09195e-02, -9.04816e-02, -1.28310e-01, -2.44387e-01, -2.48642e-01],\n",
       "          [ 1.42138e-01,  2.21218e-02, -1.87744e-01, -1.89074e-01, -3.14591e-01, -2.58348e-01],\n",
       "          [ 1.12819e-01,  8.81714e-03, -2.09284e-01, -2.35744e-01, -3.78946e-01, -3.29483e-01],\n",
       "          [ 2.66725e-01,  1.92132e-01, -1.89265e-03, -4.92962e-02, -2.04232e-01, -1.28110e-01]],\n",
       "\n",
       "         [[ 2.00775e-01,  1.50515e-01,  8.11077e-02,  6.19942e-02,  3.84140e-03, -1.15013e-01],\n",
       "          [ 1.13617e-01,  4.49084e-02, -1.57479e-02, -3.49029e-03, -5.02508e-04, -7.76507e-02],\n",
       "          [ 8.31022e-02,  1.41191e-02, -1.39346e-01, -2.37606e-01, -2.18060e-01, -1.81894e-01],\n",
       "          [ 9.38722e-02, -3.23766e-02, -2.57550e-01, -3.12198e-01, -2.92254e-01, -1.92664e-01],\n",
       "          [ 1.05772e-01,  1.27894e-02, -2.07157e-01, -2.67788e-01, -2.53694e-01, -1.60753e-01],\n",
       "          [ 1.91068e-01,  1.15944e-01, -6.66812e-02, -1.32698e-01, -1.43733e-01, -3.44375e-02]],\n",
       "\n",
       "         [[ 1.91999e-01,  9.46700e-02,  6.55510e-02,  2.24542e-02, -2.78558e-02, -1.46260e-01],\n",
       "          [ 1.31235e-01,  4.34126e-02,  2.02603e-02,  1.44681e-02,  1.21495e-02, -7.77836e-02],\n",
       "          [ 1.49318e-01,  5.70413e-02, -4.76342e-02, -1.70991e-01, -1.55966e-01, -1.44531e-01],\n",
       "          [ 1.37085e-01, -5.95427e-03, -2.15534e-01, -3.19910e-01, -3.23633e-01, -2.42525e-01],\n",
       "          [ 1.75113e-01,  3.61661e-02, -1.81495e-01, -3.18049e-01, -3.26026e-01, -2.35877e-01],\n",
       "          [ 2.46647e-01,  1.33695e-01, -4.98613e-02, -1.77373e-01, -2.00110e-01, -9.21437e-02]]],\n",
       "\n",
       "\n",
       "        [[[-9.43150e-01, -3.84597e-01,  7.25949e-01,  1.68003e-01,  2.18538e-01, -1.32775e-01],\n",
       "          [-1.76992e-01,  3.36735e-01,  1.40622e+00,  2.15355e+00,  7.12343e-01, -8.34792e-01],\n",
       "          [ 4.98543e-01,  1.15646e+00,  3.06512e+00,  2.32653e+00,  2.39845e+00, -5.75803e-01],\n",
       "          [ 2.86201e-01,  1.23810e+00,  3.63655e+00, -1.48591e+00,  6.34112e-01, -1.00486e+00],\n",
       "          [ 7.03354e-02, -6.29739e-01, -2.19024e-01, -2.12440e+00, -1.46939e+00, -1.91157e+00],\n",
       "          [-6.96794e-01, -1.26142e+00, -1.96988e+00, -1.31293e+00, -1.50729e+00, -1.30515e+00]],\n",
       "\n",
       "         [[-1.80637e-01,  2.75997e-01, -4.22012e-01, -1.20894e+00, -9.63072e-01,  8.28961e-01],\n",
       "          [-5.78232e-01, -1.32070e+00, -2.04859e+00, -7.87659e-01, -6.72499e-01,  1.17687e+00],\n",
       "          [-2.40817e+00, -2.77357e+00, -1.62780e+00, -5.03888e-01,  2.37318e+00,  2.72109e+00],\n",
       "          [-3.06512e+00, -2.23324e+00,  8.29447e-01, -2.31487e+00,  2.22158e+00,  3.18173e+00],\n",
       "          [-1.57629e+00, -1.26822e+00,  4.96113e-01,  2.61419e-01,  1.89505e+00,  1.85131e+00],\n",
       "          [-6.09513e-02,  6.82703e-01,  9.84452e-01,  2.28183e+00,  1.40428e+00,  8.83383e-01]],\n",
       "\n",
       "         [[ 8.04666e-01,  1.04179e+00,  2.62391e-01, -2.95676e-01, -5.04860e-01, -3.16570e-01],\n",
       "          [ 1.97862e+00,  1.29155e+00, -8.39165e-01, -9.71819e-01, -1.68513e+00, -1.47814e+00],\n",
       "          [ 2.27211e+00,  5.24296e-01, -8.25560e-01, -1.13411e+00, -6.54034e-01, -2.02916e+00],\n",
       "          [ 1.77551e+00,  2.20117e-01,  8.17300e-01, -1.73178e+00,  3.62610e-02, -1.47619e+00],\n",
       "          [ 1.41011e+00,  4.43878e-01,  9.62101e-01,  3.09038e-01,  1.50632e-01, -1.59281e+00],\n",
       "          [ 1.28377e+00,  8.68806e-01,  5.17008e-01,  8.93587e-01, -3.65890e-01, -1.49174e+00]]]], device='cuda:0')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.model.model[0].conv.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.model.model.0.conv.weight\n",
      "NotFound model.model.model.0.conv.bias\n",
      "model.model.model.1.conv.weight\n",
      "NotFound model.model.model.1.conv.bias\n",
      "model.model.model.2.cv1.conv.weight\n",
      "model.model.model.2.cv1.conv.bias\n",
      "model.model.model.2.cv2.conv.weight\n",
      "model.model.model.2.cv2.conv.bias\n",
      "model.model.model.2.cv3.conv.weight\n",
      "model.model.model.2.cv3.conv.bias\n",
      "model.model.model.2.m.0.cv1.conv.weight\n",
      "model.model.model.2.m.0.cv1.conv.bias\n",
      "model.model.model.2.m.0.cv2.conv.weight\n",
      "model.model.model.2.m.0.cv2.conv.bias\n",
      "model.model.model.3.conv.weight\n",
      "NotFound model.model.model.3.conv.bias\n",
      "model.model.model.4.cv1.conv.weight\n",
      "model.model.model.4.cv1.conv.bias\n",
      "model.model.model.4.cv2.conv.weight\n",
      "model.model.model.4.cv2.conv.bias\n",
      "model.model.model.4.cv3.conv.weight\n",
      "model.model.model.4.cv3.conv.bias\n",
      "model.model.model.4.m.0.cv1.conv.weight\n",
      "model.model.model.4.m.0.cv1.conv.bias\n",
      "model.model.model.4.m.0.cv2.conv.weight\n",
      "model.model.model.4.m.0.cv2.conv.bias\n",
      "model.model.model.4.m.1.cv1.conv.weight\n",
      "model.model.model.4.m.1.cv1.conv.bias\n",
      "model.model.model.4.m.1.cv2.conv.weight\n",
      "model.model.model.4.m.1.cv2.conv.bias\n",
      "model.model.model.5.conv.weight\n",
      "NotFound model.model.model.5.conv.bias\n",
      "model.model.model.6.cv1.conv.weight\n",
      "model.model.model.6.cv1.conv.bias\n",
      "model.model.model.6.cv2.conv.weight\n",
      "model.model.model.6.cv2.conv.bias\n",
      "model.model.model.6.cv3.conv.weight\n",
      "model.model.model.6.cv3.conv.bias\n",
      "model.model.model.6.m.0.cv1.conv.weight\n",
      "model.model.model.6.m.0.cv1.conv.bias\n",
      "model.model.model.6.m.0.cv2.conv.weight\n",
      "model.model.model.6.m.0.cv2.conv.bias\n",
      "model.model.model.6.m.1.cv1.conv.weight\n",
      "model.model.model.6.m.1.cv1.conv.bias\n",
      "model.model.model.6.m.1.cv2.conv.weight\n",
      "model.model.model.6.m.1.cv2.conv.bias\n",
      "model.model.model.6.m.2.cv1.conv.weight\n",
      "model.model.model.6.m.2.cv1.conv.bias\n",
      "model.model.model.6.m.2.cv2.conv.weight\n",
      "model.model.model.6.m.2.cv2.conv.bias\n",
      "model.model.model.7.conv.weight\n",
      "NotFound model.model.model.7.conv.bias\n",
      "model.model.model.8.cv1.conv.weight\n",
      "model.model.model.8.cv1.conv.bias\n",
      "model.model.model.8.cv2.conv.weight\n",
      "model.model.model.8.cv2.conv.bias\n",
      "model.model.model.8.cv3.conv.weight\n",
      "model.model.model.8.cv3.conv.bias\n",
      "model.model.model.8.m.0.cv1.conv.weight\n",
      "model.model.model.8.m.0.cv1.conv.bias\n",
      "model.model.model.8.m.0.cv2.conv.weight\n",
      "model.model.model.8.m.0.cv2.conv.bias\n",
      "model.model.model.9.cv1.conv.weight\n",
      "model.model.model.9.cv1.conv.bias\n",
      "model.model.model.9.cv2.conv.weight\n",
      "model.model.model.9.cv2.conv.bias\n",
      "model.model.model.10.conv.weight\n",
      "NotFound model.model.model.10.conv.bias\n",
      "model.model.model.13.cv1.conv.weight\n",
      "model.model.model.13.cv1.conv.bias\n",
      "model.model.model.13.cv2.conv.weight\n",
      "model.model.model.13.cv2.conv.bias\n",
      "model.model.model.13.cv3.conv.weight\n",
      "model.model.model.13.cv3.conv.bias\n",
      "model.model.model.13.m.0.cv1.conv.weight\n",
      "model.model.model.13.m.0.cv1.conv.bias\n",
      "model.model.model.13.m.0.cv2.conv.weight\n",
      "model.model.model.13.m.0.cv2.conv.bias\n",
      "model.model.model.14.conv.weight\n",
      "NotFound model.model.model.14.conv.bias\n",
      "model.model.model.17.cv1.conv.weight\n",
      "model.model.model.17.cv1.conv.bias\n",
      "model.model.model.17.cv2.conv.weight\n",
      "model.model.model.17.cv2.conv.bias\n",
      "model.model.model.17.cv3.conv.weight\n",
      "model.model.model.17.cv3.conv.bias\n",
      "model.model.model.17.m.0.cv1.conv.weight\n",
      "model.model.model.17.m.0.cv1.conv.bias\n",
      "model.model.model.17.m.0.cv2.conv.weight\n",
      "model.model.model.17.m.0.cv2.conv.bias\n",
      "model.model.model.18.conv.weight\n",
      "NotFound model.model.model.18.conv.bias\n",
      "model.model.model.20.cv1.conv.weight\n",
      "model.model.model.20.cv1.conv.bias\n",
      "model.model.model.20.cv2.conv.weight\n",
      "model.model.model.20.cv2.conv.bias\n",
      "model.model.model.20.cv3.conv.weight\n",
      "model.model.model.20.cv3.conv.bias\n",
      "model.model.model.20.m.0.cv1.conv.weight\n",
      "model.model.model.20.m.0.cv1.conv.bias\n",
      "model.model.model.20.m.0.cv2.conv.weight\n",
      "model.model.model.20.m.0.cv2.conv.bias\n",
      "model.model.model.21.conv.weight\n",
      "NotFound model.model.model.21.conv.bias\n",
      "model.model.model.23.cv1.conv.weight\n",
      "model.model.model.23.cv1.conv.bias\n",
      "model.model.model.23.cv2.conv.weight\n",
      "model.model.model.23.cv2.conv.bias\n",
      "model.model.model.23.cv3.conv.weight\n",
      "model.model.model.23.cv3.conv.bias\n",
      "model.model.model.23.m.0.cv1.conv.weight\n",
      "model.model.model.23.m.0.cv1.conv.bias\n",
      "model.model.model.23.m.0.cv2.conv.weight\n",
      "model.model.model.23.m.0.cv2.conv.bias\n",
      "model.model.model.24.anchors\n",
      "model.model.model.24.m.0.weight\n",
      "model.model.model.24.m.0.bias\n",
      "model.model.model.24.m.1.weight\n",
      "model.model.model.24.m.1.bias\n",
      "model.model.model.24.m.2.weight\n",
      "model.model.model.24.m.2.bias\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_quant_state = model_quant.state_dict()\n",
    "model_state = model.state_dict()\n",
    "\n",
    "for key,value in model_state.items():\n",
    "    if key in model_quant_state:\n",
    "        print(key)\n",
    "        model_quant_state[key] = value\n",
    "    else:\n",
    "        print(\"NotFound\",key)\n",
    "        \n",
    "model_quant.load_state_dict(model_quant_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model_quant(im)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 3.82984e-03,  1.28975e-01,  1.35297e-01,  2.92381e-01,  1.65547e-01,  1.44634e-01],\n",
       "          [ 9.60014e-02,  2.49973e-01,  2.84989e-01,  3.86146e-01,  2.49779e-01,  1.71480e-01],\n",
       "          [ 6.13261e-02,  2.40636e-01,  3.78559e-01,  4.74268e-01,  2.02508e-01,  7.06150e-02],\n",
       "          [ 7.99039e-02,  1.92197e-01,  2.95299e-01,  3.52881e-01,  5.88459e-02, -4.01465e-02],\n",
       "          [-1.49595e-01, -7.84449e-02, -4.27240e-02, -1.40306e-02, -2.46472e-01, -3.21756e-01],\n",
       "          [-2.78570e-01, -2.76819e-01, -3.36735e-01, -2.39858e-01, -3.64747e-01, -3.45099e-01]],\n",
       "\n",
       "         [[-2.56296e-02,  1.49546e-02, -2.90339e-02,  1.05631e-01,  4.50584e-02,  1.15746e-01],\n",
       "          [ 9.69255e-02,  1.86264e-01,  1.61072e-01,  2.30520e-01,  1.24209e-01,  1.29947e-01],\n",
       "          [ 1.13315e-01,  2.28575e-01,  3.01135e-01,  3.47628e-01,  9.11381e-02,  3.52589e-02],\n",
       "          [ 1.34908e-01,  1.69924e-01,  2.02508e-01,  2.22350e-01, -7.81045e-02, -8.67125e-02],\n",
       "          [-8.31137e-02, -7.97094e-02, -1.00865e-01, -1.02810e-01, -3.53853e-01, -3.67471e-01],\n",
       "          [-9.28403e-02, -1.44148e-01, -2.35383e-01, -1.37728e-01, -3.03859e-01, -2.59894e-01]],\n",
       "\n",
       "         [[ 2.89852e-02,  9.96975e-02,  7.65483e-02,  1.91030e-01,  1.09035e-01,  1.34713e-01],\n",
       "          [ 1.15552e-01,  2.18265e-01,  2.01535e-01,  2.48028e-01,  1.27418e-01,  1.13607e-01],\n",
       "          [ 9.89680e-02,  2.06787e-01,  2.86934e-01,  3.00163e-01,  6.59463e-02,  3.47422e-03],\n",
       "          [ 1.24792e-01,  1.38020e-01,  1.68367e-01,  1.79747e-01, -8.98250e-02, -7.90772e-02],\n",
       "          [-5.62197e-02, -7.05178e-02, -7.86881e-02, -7.98067e-02, -2.69232e-01, -2.70010e-01],\n",
       "          [-1.35321e-02, -6.86211e-02, -1.36464e-01, -3.25598e-02, -1.70313e-01, -1.26640e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 1.57730e-02, -1.32307e-01, -9.66445e-02, -5.94099e-02,  1.02271e-01,  7.30212e-02],\n",
       "          [ 4.11070e-01,  1.57957e-01,  1.63253e-01, -6.92398e-01, -5.65139e-02,  1.14269e-01],\n",
       "          [-1.93951e-01, -1.55161e+00,  5.71262e-01,  2.29531e-01, -4.34238e-01, -5.56699e-01],\n",
       "          [-2.77754e+00, -3.50303e+00,  1.93289e+00,  4.15704e+00,  1.81639e+00, -9.16799e-02],\n",
       "          [-1.55690e+00, -2.50349e+00,  8.29422e-01,  3.30180e+00,  1.21799e+00,  9.50724e-02],\n",
       "          [ 7.28143e-01, -4.97785e-01,  2.73219e-01, -1.68631e-01, -7.87057e-01, -3.92866e-01]],\n",
       "\n",
       "         [[ 4.53435e-01, -1.49518e-01,  8.99423e-02, -2.39625e-01, -3.04000e-01, -3.88233e-01],\n",
       "          [ 5.06060e-01, -1.73100e-01, -2.25269e-02, -9.39967e-01, -4.32252e-01, -2.82321e-01],\n",
       "          [-3.08468e-01, -1.86405e+00,  1.00616e+00,  7.80437e-01, -2.22249e-01, -4.17028e-01],\n",
       "          [-3.77311e+00, -4.42446e+00,  2.23077e+00,  5.30618e+00,  2.52467e+00,  4.35893e-01],\n",
       "          [-2.72988e+00, -3.52157e+00,  8.65829e-01,  3.84460e+00,  1.55161e+00,  2.45417e-01],\n",
       "          [ 3.79297e-01, -8.96940e-01,  4.55752e-01,  2.51044e-01, -4.76272e-01, -3.10123e-01]],\n",
       "\n",
       "         [[ 1.84208e-02, -1.23453e-01, -1.73100e-01,  2.94898e-01,  2.57663e-01,  1.27094e-01],\n",
       "          [ 4.72962e-01,  1.96930e-01,  1.40002e-01, -4.83222e-01, -6.78497e-03,  2.25890e-01],\n",
       "          [ 8.71124e-01, -5.72255e-01,  8.90983e-01,  9.65618e-02, -5.76226e-01, -1.17248e-01],\n",
       "          [-9.55192e-01, -2.61205e+00,  1.07169e+00,  2.19767e+00,  1.08229e-01, -5.15658e-01],\n",
       "          [-3.84923e-01, -1.90112e+00, -4.06436e-01,  1.39671e+00, -7.75721e-06, -3.65065e-01],\n",
       "          [ 1.37553e+00,  1.77795e-02,  1.62343e-01, -2.42108e-01, -3.63079e-01,  8.31159e-02]]],\n",
       "\n",
       "\n",
       "        [[[ 4.86383e-01,  6.07766e-01,  9.77009e-01,  1.05680e+00,  8.70056e-01,  6.14557e-01],\n",
       "          [ 3.95133e-01,  5.76359e-01,  8.27614e-01,  8.41196e-01,  8.69207e-01,  5.60656e-01],\n",
       "          [ 5.91532e-02,  1.74436e-01,  1.83667e-01,  2.15790e-02,  2.32369e-01,  2.16984e-01],\n",
       "          [-1.39527e-01, -2.22819e-01, -2.27063e-01, -3.51206e-01, -2.96668e-01, -1.19686e-01],\n",
       "          [-2.55924e-01, -4.91900e-01, -8.41196e-01, -1.10688e+00, -9.71068e-01, -3.88979e-01],\n",
       "          [-3.14281e-01, -4.57522e-01, -8.85335e-01, -1.24015e+00, -1.01860e+00, -4.11048e-01]],\n",
       "\n",
       "         [[ 2.79903e-01,  3.74336e-01,  7.10900e-01,  8.15306e-01,  6.82888e-01,  3.93223e-01],\n",
       "          [ 2.93909e-01,  4.40121e-01,  6.71853e-01,  7.08353e-01,  8.00027e-01,  4.51580e-01],\n",
       "          [ 7.02411e-02,  1.68494e-01,  1.56822e-01,  2.26533e-02,  2.93909e-01,  2.22607e-01],\n",
       "          [-8.58915e-02, -1.92686e-01, -1.98734e-01, -2.89877e-01, -1.90245e-01, -7.89417e-02],\n",
       "          [-2.16241e-01, -4.43516e-01, -7.59283e-01, -9.81254e-01, -8.26341e-01, -3.15767e-01],\n",
       "          [-2.06055e-01, -3.20860e-01, -7.03260e-01, -1.01521e+00, -7.89417e-01, -2.70142e-01]],\n",
       "\n",
       "         [[ 1.18837e-01,  1.96930e-01,  4.81714e-01,  5.88667e-01,  4.42668e-01,  2.48284e-01],\n",
       "          [ 1.49713e-01,  2.66746e-01,  4.36301e-01,  4.82987e-01,  5.30522e-01,  2.81813e-01],\n",
       "          [ 5.09036e-02,  1.32206e-01,  1.06688e-01, -1.16052e-02,  1.85046e-01,  1.44939e-01],\n",
       "          [-3.22557e-02, -7.34242e-02, -3.25741e-02, -8.92338e-02, -8.82789e-02, -2.78259e-02],\n",
       "          [-9.60775e-02, -2.53802e-01, -4.62191e-01, -6.31958e-01, -5.79754e-01, -1.83242e-01],\n",
       "          [-6.63683e-02, -1.56716e-01, -4.69406e-01, -7.31272e-01, -5.65749e-01, -1.21702e-01]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[ 1.06954e-01,  5.51430e-02,  2.78374e-01,  3.24257e-01, -3.50211e-02,  1.10414e-01],\n",
       "          [ 5.74179e-02,  5.01125e-02,  4.46270e-01,  5.48546e-01,  1.00930e-02,  4.14133e-03],\n",
       "          [ 2.09806e-01,  3.21694e-01,  8.83313e-01,  1.04531e+00,  3.15286e-01,  1.34061e-01],\n",
       "          [ 3.43482e-01,  5.83920e-01,  1.19347e+00,  1.33804e+00,  5.81869e-01,  3.06571e-01],\n",
       "          [-1.45387e-02,  1.04134e-01,  6.02375e-01,  7.63863e-01,  2.82732e-01,  1.41110e-01],\n",
       "          [ 9.31759e-02,  1.06633e-01,  3.21438e-01,  4.27558e-01,  9.39449e-02,  1.27396e-01]],\n",
       "\n",
       "         [[ 1.58027e-01, -3.49250e-02, -2.64180e-02, -1.20475e-02, -1.49569e-01,  2.04551e-01],\n",
       "          [-8.27305e-02, -2.75298e-01, -5.74820e-02,  5.09135e-02, -2.59534e-01, -7.17724e-02],\n",
       "          [-1.37521e-01, -1.88274e-01,  2.51460e-01,  4.77799e-01, -7.72835e-02, -1.20411e-01],\n",
       "          [-4.87668e-02,  4.87347e-02,  5.70078e-01,  7.57199e-01,  1.29190e-01, -4.77735e-02],\n",
       "          [-3.04776e-01, -2.82476e-01,  1.21116e-01,  3.05289e-01, -1.96253e-02, -6.77352e-02],\n",
       "          [-1.02532e-01, -1.48671e-01, -3.59503e-02,  7.75398e-02, -8.80493e-02,  2.34862e-02]],\n",
       "\n",
       "         [[-2.44539e-01, -3.33998e-01, -2.89140e-01, -2.28646e-01, -3.92954e-01, -1.93785e-01],\n",
       "          [-2.49922e-01, -3.00675e-01,  6.67099e-02,  2.24930e-01, -1.65717e-01, -2.27621e-01],\n",
       "          [-1.55208e-01, -1.60046e-02,  6.25445e-01,  9.53035e-01,  2.59150e-01, -7.35667e-02],\n",
       "          [-6.39864e-02,  1.86736e-01,  8.67933e-01,  1.13913e+00,  3.50659e-01, -8.03595e-02],\n",
       "          [-3.77318e-01, -2.61585e-01,  2.21085e-01,  4.34992e-01, -1.22558e-02, -2.11472e-01],\n",
       "          [-2.84526e-01, -3.11441e-01, -2.09550e-01, -8.01672e-02, -3.05802e-01, -2.59021e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 3.12198e-01,  2.90126e-01,  2.45451e-01,  2.77362e-01,  9.42711e-02, -7.42601e-02],\n",
       "          [ 1.80032e-01,  1.26249e-01,  8.37670e-02,  1.60886e-01,  2.87035e-02, -8.86866e-02],\n",
       "          [ 1.12952e-01,  4.09195e-02, -9.04816e-02, -1.28310e-01, -2.44387e-01, -2.48642e-01],\n",
       "          [ 1.42138e-01,  2.21218e-02, -1.87744e-01, -1.89074e-01, -3.14591e-01, -2.58348e-01],\n",
       "          [ 1.12819e-01,  8.81714e-03, -2.09284e-01, -2.35744e-01, -3.78946e-01, -3.29483e-01],\n",
       "          [ 2.66725e-01,  1.92132e-01, -1.89265e-03, -4.92962e-02, -2.04232e-01, -1.28110e-01]],\n",
       "\n",
       "         [[ 2.00775e-01,  1.50515e-01,  8.11077e-02,  6.19942e-02,  3.84140e-03, -1.15013e-01],\n",
       "          [ 1.13617e-01,  4.49084e-02, -1.57479e-02, -3.49029e-03, -5.02508e-04, -7.76507e-02],\n",
       "          [ 8.31022e-02,  1.41191e-02, -1.39346e-01, -2.37606e-01, -2.18060e-01, -1.81894e-01],\n",
       "          [ 9.38722e-02, -3.23766e-02, -2.57550e-01, -3.12198e-01, -2.92254e-01, -1.92664e-01],\n",
       "          [ 1.05772e-01,  1.27894e-02, -2.07157e-01, -2.67788e-01, -2.53694e-01, -1.60753e-01],\n",
       "          [ 1.91068e-01,  1.15944e-01, -6.66812e-02, -1.32698e-01, -1.43733e-01, -3.44375e-02]],\n",
       "\n",
       "         [[ 1.91999e-01,  9.46700e-02,  6.55510e-02,  2.24542e-02, -2.78558e-02, -1.46260e-01],\n",
       "          [ 1.31235e-01,  4.34126e-02,  2.02603e-02,  1.44681e-02,  1.21495e-02, -7.77836e-02],\n",
       "          [ 1.49318e-01,  5.70413e-02, -4.76342e-02, -1.70991e-01, -1.55966e-01, -1.44531e-01],\n",
       "          [ 1.37085e-01, -5.95427e-03, -2.15534e-01, -3.19910e-01, -3.23633e-01, -2.42525e-01],\n",
       "          [ 1.75113e-01,  3.61661e-02, -1.81495e-01, -3.18049e-01, -3.26026e-01, -2.35877e-01],\n",
       "          [ 2.46647e-01,  1.33695e-01, -4.98613e-02, -1.77373e-01, -2.00110e-01, -9.21437e-02]]],\n",
       "\n",
       "\n",
       "        [[[-9.43150e-01, -3.84597e-01,  7.25949e-01,  1.68003e-01,  2.18538e-01, -1.32775e-01],\n",
       "          [-1.76992e-01,  3.36735e-01,  1.40622e+00,  2.15355e+00,  7.12343e-01, -8.34792e-01],\n",
       "          [ 4.98543e-01,  1.15646e+00,  3.06512e+00,  2.32653e+00,  2.39845e+00, -5.75803e-01],\n",
       "          [ 2.86201e-01,  1.23810e+00,  3.63655e+00, -1.48591e+00,  6.34112e-01, -1.00486e+00],\n",
       "          [ 7.03354e-02, -6.29739e-01, -2.19024e-01, -2.12440e+00, -1.46939e+00, -1.91157e+00],\n",
       "          [-6.96794e-01, -1.26142e+00, -1.96988e+00, -1.31293e+00, -1.50729e+00, -1.30515e+00]],\n",
       "\n",
       "         [[-1.80637e-01,  2.75997e-01, -4.22012e-01, -1.20894e+00, -9.63072e-01,  8.28961e-01],\n",
       "          [-5.78232e-01, -1.32070e+00, -2.04859e+00, -7.87659e-01, -6.72499e-01,  1.17687e+00],\n",
       "          [-2.40817e+00, -2.77357e+00, -1.62780e+00, -5.03888e-01,  2.37318e+00,  2.72109e+00],\n",
       "          [-3.06512e+00, -2.23324e+00,  8.29447e-01, -2.31487e+00,  2.22158e+00,  3.18173e+00],\n",
       "          [-1.57629e+00, -1.26822e+00,  4.96113e-01,  2.61419e-01,  1.89505e+00,  1.85131e+00],\n",
       "          [-6.09513e-02,  6.82703e-01,  9.84452e-01,  2.28183e+00,  1.40428e+00,  8.83383e-01]],\n",
       "\n",
       "         [[ 8.04666e-01,  1.04179e+00,  2.62391e-01, -2.95676e-01, -5.04860e-01, -3.16570e-01],\n",
       "          [ 1.97862e+00,  1.29155e+00, -8.39165e-01, -9.71819e-01, -1.68513e+00, -1.47814e+00],\n",
       "          [ 2.27211e+00,  5.24296e-01, -8.25560e-01, -1.13411e+00, -6.54034e-01, -2.02916e+00],\n",
       "          [ 1.77551e+00,  2.20117e-01,  8.17300e-01, -1.73178e+00,  3.62610e-02, -1.47619e+00],\n",
       "          [ 1.41011e+00,  4.43878e-01,  9.62101e-01,  3.09038e-01,  1.50632e-01, -1.59281e+00],\n",
       "          [ 1.28377e+00,  8.68806e-01,  5.17008e-01,  8.93587e-01, -3.65890e-01, -1.49174e+00]]]], device='cuda:0')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_quant.model.model.model[0].conv.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_quant.load_state_dict(model.state_dict(),strict = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_quant.model.model.model[0].conv.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name,weights in model.state_dict().items():\n",
    "    print(name)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "YOLOv5 Tutorial",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
