{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7mGmQbAO5pQb"
   },
   "source": [
    "# Setup\n",
    "\n",
    "Clone GitHub [repository](https://github.com/ultralytics/yolov5), install [dependencies](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) and check PyTorch and GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wbvMlHd_QwMG",
    "outputId": "e8225db4-e61d-4640-8b1f-8bfce3331cea"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import utils\n",
    "display = utils.notebook_init()  # checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4JnkELT0cIJg"
   },
   "source": [
    "# 1. Detect\n",
    "\n",
    "`detect.py` runs YOLOv5 inference on a variety of sources, downloading models automatically from the [latest YOLOv5 release](https://github.com/ultralytics/yolov5/releases), and saving results to `runs/detect`. Example inference sources are:\n",
    "\n",
    "```shell\n",
    "python detect.py --source 0  # webcam\n",
    "                          img.jpg  # image\n",
    "                          vid.mp4  # video\n",
    "                          screen  # screenshot\n",
    "                          path/  # directory\n",
    "                         'path/*.jpg'  # glob\n",
    "                         'https://youtu.be/LNwODJXcvt4'  # YouTube\n",
    "                         'rtsp://example.com/media.mp4'  # RTSP, RTMP, HTTP stream\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detect Unquantized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 detect.py --cfg models/lpyolo.yaml --weights experiment_models/lpyolo.pt --img 640 --conf 0.25 --source data/images/zidane.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detect Quantized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zR9ZbuQCH7FX",
    "outputId": "284ef04b-1596-412f-88f6-948828dd2b49"
   },
   "outputs": [],
   "source": [
    "!python3 detect.py --cfg models/lpyolo_quant.yaml --weights experiment_models/lpyolo_W4A4.pt --img 640 --conf 0.25 --source data/images/zidane.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0eq1SMWl6Sfn"
   },
   "source": [
    "# 2. Validate\n",
    "Validate a model's accuracy on the [COCO](https://cocodataset.org/#home) dataset's `val` or `test` splits. Models are downloaded automatically from the [latest YOLOv5 release](https://github.com/ultralytics/yolov5/releases). To show results by class use the `--verbose` flag."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate Unquantized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 val.py --weights experiment_models/lpyolo_coco_anchors_unnorm.pt --data VOC.yaml --img 640 --half"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate Quantized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X58w8JLpMnjH",
    "outputId": "3e234e05-ee8b-4ad1-b1a4-f6a55d5e4f3d"
   },
   "outputs": [],
   "source": [
    "!python3 val.py --weights experiment_models/lpyolo_W4A4.pt --data coco128.yaml --img 640 --half"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Unquantized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 train.py --img 640 --batch 64 --epochs 300 --data coco128.yaml --weights '' --cache --cfg models/lpyolo.yaml --classes 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Quantized (QAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1NcFxRcFdJ_O",
    "outputId": "bbeeea2b-04fc-4185-aa64-258690495b5a"
   },
   "outputs": [],
   "source": [
    "!python3 train.py --img 640 --batch 32 --epochs 50 --data coco128.yaml --weights experiment_models/lpyolo_coco_anchors_unnorm.pt --cache --cfg models/lpyolo_quant.yaml --classes 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 ðŸš€ v7.0-268-g0333f718 Python-3.10.12 torch-2.1.2+cu121 CPU\n",
      "\n",
      "Channels [3]\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1       232  models.common.Conv                      [3, 8, 3, 1]                  \n",
      "  1                -1  1         0  torch.nn.modules.pooling.MaxPool2d      [2, 2, 0]                     \n",
      "  2                -1  1       592  models.common.Conv                      [8, 8, 3, 1]                  \n",
      "  3                -1  1         0  torch.nn.modules.pooling.MaxPool2d      [2, 2, 0]                     \n",
      "  4                -1  1      1184  models.common.Conv                      [8, 16, 3, 1]                 \n",
      "  5                -1  1         0  torch.nn.modules.pooling.MaxPool2d      [2, 2, 0]                     \n",
      "  6                -1  1      4672  models.common.Conv                      [16, 32, 3, 1]                \n",
      "  7                -1  1         0  torch.nn.modules.pooling.MaxPool2d      [2, 2, 0]                     \n",
      "  8                -1  1     16240  models.common.Conv                      [32, 56, 3, 1]                \n",
      "  9                -1  1         0  torch.nn.modules.pooling.MaxPool2d      [2, 2, 0]                     \n",
      " 10                -1  1     52624  models.common.Conv                      [56, 104, 3, 1]               \n",
      " 11                -1  1    195104  models.common.Conv                      [104, 208, 3, 1]              \n",
      " 12                -1  1     11760  models.common.Conv                      [208, 56, 1, 1]               \n",
      " 13                -1  1     52624  models.common.Conv                      [56, 104, 3, 1]               \n",
      " 14                -1  1     33696  models.common.SimpleConv                [104, 36, 3, 1]               \n",
      " 15      [14, 14, 14]  1      3996  models.yolo.Detect                      [7, [[10, 13, 16, 30, 33, 23], [81, 82, 135, 169, 344, 319], [116, 90, 156, 198, 373, 326]], [36, 36, 36], False]\n",
      "Channels [3]\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1       232  models.common.Conv                      [3, 8, 3, 1]                  \n",
      "  1                -1  1         0  torch.nn.modules.pooling.MaxPool2d      [2, 2, 0]                     \n",
      "  2                -1  1       592  models.common.Conv                      [8, 8, 3, 1]                  \n",
      "  3                -1  1         0  torch.nn.modules.pooling.MaxPool2d      [2, 2, 0]                     \n",
      "  4                -1  1      1184  models.common.Conv                      [8, 16, 3, 1]                 \n",
      "  5                -1  1         0  torch.nn.modules.pooling.MaxPool2d      [2, 2, 0]                     \n",
      "  6                -1  1      4672  models.common.Conv                      [16, 32, 3, 1]                \n",
      "  7                -1  1         0  torch.nn.modules.pooling.MaxPool2d      [2, 2, 0]                     \n",
      "  8                -1  1     16240  models.common.Conv                      [32, 56, 3, 1]                \n",
      "  9                -1  1         0  torch.nn.modules.pooling.MaxPool2d      [2, 2, 0]                     \n",
      " 10                -1  1     52624  models.common.Conv                      [56, 104, 3, 1]               \n",
      " 11                -1  1    195104  models.common.Conv                      [104, 208, 3, 1]              \n",
      " 12                -1  1     11760  models.common.Conv                      [208, 56, 1, 1]               \n",
      " 13                -1  1     52624  models.common.Conv                      [56, 104, 3, 1]               \n",
      " 14                -1  1     33696  models.common.SimpleConv                [104, 36, 3, 1]               \n",
      " 15      [14, 14, 14]  1      3996  models.yolo.Detect                      [7, [[10, 13, 16, 30, 33, 23], [81, 82, 135, 169, 344, 319], [116, 90, 156, 198, 373, 326]], [36, 36, 36], False]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 804: forward compatibility was attempted on non supported HW\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 804: forward compatibility was attempted on non supported HW. Cache may be out of date, try `force_reload=True` or see https://docs.ultralytics.com/yolov5/tutorials/pytorch_hub_model_loading for help.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/quant_yolov5/./hubconf.py:53\u001b[0m, in \u001b[0;36m_create\u001b[0;34m(name, pretrained, channels, classes, autoshape, verbose, device, cfg)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 53\u001b[0m     model \u001b[39m=\u001b[39m DetectMultiBackend(path, device\u001b[39m=\u001b[39;49mdevice, fuse\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)  \u001b[39m# detection model\u001b[39;00m\n\u001b[1;32m     54\u001b[0m     \u001b[39mif\u001b[39;00m autoshape:\n",
      "File \u001b[0;32m~/quant_yolov5/./models/common.py:553\u001b[0m, in \u001b[0;36mDetectMultiBackend.__init__\u001b[0;34m(self, weights, device, dnn, data, fp16, fuse)\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[39mif\u001b[39;00m pt:  \u001b[39m# PyTorch\u001b[39;00m\n\u001b[0;32m--> 553\u001b[0m     model \u001b[39m=\u001b[39m attempt_load(weights \u001b[39mif\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(weights, \u001b[39mlist\u001b[39;49m) \u001b[39melse\u001b[39;49;00m w, device, \u001b[39mTrue\u001b[39;49;00m,fuse)\n\u001b[1;32m    554\u001b[0m     stride \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(\u001b[39mint\u001b[39m(model\u001b[39m.\u001b[39mstride\u001b[39m.\u001b[39mmax()), \u001b[39m32\u001b[39m)  \u001b[39m# model stride\u001b[39;00m\n",
      "File \u001b[0;32m~/quant_yolov5/./models/experimental.py:89\u001b[0m, in \u001b[0;36mattempt_load\u001b[0;34m(weights, device, inplace, fuse, cfg)\u001b[0m\n\u001b[1;32m     87\u001b[0m     nc \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(nc)\n\u001b[0;32m---> 89\u001b[0m ckpt \u001b[39m=\u001b[39m Model(cfg, ch\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m, nc\u001b[39m=\u001b[39;49mnc)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     90\u001b[0m \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m weights \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(weights, \u001b[39mlist\u001b[39m) \u001b[39melse\u001b[39;00m [weights]:\n",
      "File \u001b[0;32m~/quant_yolov5/./models/yolo.py:214\u001b[0m, in \u001b[0;36mDetectionModel.__init__\u001b[0;34m(self, cfg, ch, nc, anchors)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave \u001b[39m=\u001b[39m parse_model(deepcopy(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39myaml), ch\u001b[39m=\u001b[39m[ch])  \u001b[39m# model, savelist\u001b[39;00m\n\u001b[0;32m--> 214\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mcuda()\n\u001b[1;32m    215\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnames \u001b[39m=\u001b[39m [\u001b[39mstr\u001b[39m(i) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39myaml[\u001b[39m'\u001b[39m\u001b[39mnc\u001b[39m\u001b[39m'\u001b[39m])]  \u001b[39m# default names\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:918\u001b[0m, in \u001b[0;36mModule.cuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    902\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Moves all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[1;32m    903\u001b[0m \n\u001b[1;32m    904\u001b[0m \u001b[39mThis also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    916\u001b[0m \u001b[39m    Module: self\u001b[39;00m\n\u001b[1;32m    917\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 918\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(\u001b[39mlambda\u001b[39;49;00m t: t\u001b[39m.\u001b[39;49mcuda(device))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    809\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    812\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    809\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    812\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:833\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    832\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 833\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[1;32m    834\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:918\u001b[0m, in \u001b[0;36mModule.cuda.<locals>.<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    902\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Moves all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[1;32m    903\u001b[0m \n\u001b[1;32m    904\u001b[0m \u001b[39mThis also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    916\u001b[0m \u001b[39m    Module: self\u001b[39;00m\n\u001b[1;32m    917\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 918\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_apply(\u001b[39mlambda\u001b[39;00m t: t\u001b[39m.\u001b[39;49mcuda(device))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:298\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    297\u001b[0m     os\u001b[39m.\u001b[39menviron[\u001b[39m\"\u001b[39m\u001b[39mCUDA_MODULE_LOADING\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mLAZY\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 298\u001b[0m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_cuda_init()\n\u001b[1;32m    299\u001b[0m \u001b[39m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    300\u001b[0m \u001b[39m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    301\u001b[0m \u001b[39m# However, we must not let any *other* threads in!\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 804: forward compatibility was attempted on non supported HW",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/quant_yolov5/./hubconf.py:65\u001b[0m, in \u001b[0;36m_create\u001b[0;34m(name, pretrained, channels, classes, autoshape, verbose, device, cfg)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[39mprint\u001b[39m(e)\n\u001b[0;32m---> 65\u001b[0m         model \u001b[39m=\u001b[39m attempt_load(weights \u001b[39m=\u001b[39;49m path, device\u001b[39m=\u001b[39;49mdevice, fuse\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)  \u001b[39m# arbitrary model\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/quant_yolov5/./models/experimental.py:89\u001b[0m, in \u001b[0;36mattempt_load\u001b[0;34m(weights, device, inplace, fuse, cfg)\u001b[0m\n\u001b[1;32m     87\u001b[0m     nc \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(nc)\n\u001b[0;32m---> 89\u001b[0m ckpt \u001b[39m=\u001b[39m Model(cfg, ch\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m, nc\u001b[39m=\u001b[39;49mnc)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     90\u001b[0m \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m weights \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(weights, \u001b[39mlist\u001b[39m) \u001b[39melse\u001b[39;00m [weights]:\n",
      "File \u001b[0;32m~/quant_yolov5/./models/yolo.py:214\u001b[0m, in \u001b[0;36mDetectionModel.__init__\u001b[0;34m(self, cfg, ch, nc, anchors)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave \u001b[39m=\u001b[39m parse_model(deepcopy(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39myaml), ch\u001b[39m=\u001b[39m[ch])  \u001b[39m# model, savelist\u001b[39;00m\n\u001b[0;32m--> 214\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mcuda()\n\u001b[1;32m    215\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnames \u001b[39m=\u001b[39m [\u001b[39mstr\u001b[39m(i) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39myaml[\u001b[39m'\u001b[39m\u001b[39mnc\u001b[39m\u001b[39m'\u001b[39m])]  \u001b[39m# default names\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:918\u001b[0m, in \u001b[0;36mModule.cuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    902\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Moves all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[1;32m    903\u001b[0m \n\u001b[1;32m    904\u001b[0m \u001b[39mThis also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    916\u001b[0m \u001b[39m    Module: self\u001b[39;00m\n\u001b[1;32m    917\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 918\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(\u001b[39mlambda\u001b[39;49;00m t: t\u001b[39m.\u001b[39;49mcuda(device))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    809\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    812\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    809\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    812\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:833\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    832\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 833\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[1;32m    834\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:918\u001b[0m, in \u001b[0;36mModule.cuda.<locals>.<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    902\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Moves all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[1;32m    903\u001b[0m \n\u001b[1;32m    904\u001b[0m \u001b[39mThis also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    916\u001b[0m \u001b[39m    Module: self\u001b[39;00m\n\u001b[1;32m    917\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 918\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_apply(\u001b[39mlambda\u001b[39;00m t: t\u001b[39m.\u001b[39;49mcuda(device))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:298\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    297\u001b[0m     os\u001b[39m.\u001b[39menviron[\u001b[39m\"\u001b[39m\u001b[39mCUDA_MODULE_LOADING\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mLAZY\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 298\u001b[0m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_cuda_init()\n\u001b[1;32m    299\u001b[0m \u001b[39m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    300\u001b[0m \u001b[39m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    301\u001b[0m \u001b[39m# However, we must not let any *other* threads in!\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 804: forward compatibility was attempted on non supported HW",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/omar/quant_yolov5/experiments.ipynb Cell 19\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.33.32.101/home/omar/quant_yolov5/experiments.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B10.33.32.101/home/omar/quant_yolov5/experiments.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m model_pre \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mhub\u001b[39m.\u001b[39;49mload(\u001b[39m'\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.33.32.101/home/omar/quant_yolov5/experiments.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m                       \u001b[39m'\u001b[39;49m\u001b[39mcustom\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.33.32.101/home/omar/quant_yolov5/experiments.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m                       \u001b[39m'\u001b[39;49m\u001b[39mexperiment_models/lpyolo.pt\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.33.32.101/home/omar/quant_yolov5/experiments.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m                        source\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mlocal\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.33.32.101/home/omar/quant_yolov5/experiments.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m                        classes \u001b[39m=\u001b[39;49m \u001b[39m7\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.33.32.101/home/omar/quant_yolov5/experiments.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m                        cfg \u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mmodels/lpyolo.yaml\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.33.32.101/home/omar/quant_yolov5/experiments.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m                        force_reload\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.33.32.101/home/omar/quant_yolov5/experiments.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m                       )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.33.32.101/home/omar/quant_yolov5/experiments.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m im \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m/home/omar/datasets/coco128/images/train2017/000000579736.jpg\u001b[39m\u001b[39m'\u001b[39m  \u001b[39m# file, Path, PIL.Image, OpenCV, nparray, list\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.33.32.101/home/omar/quant_yolov5/experiments.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m results \u001b[39m=\u001b[39m model_pre(im)  \u001b[39m# inference\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/hub.py:566\u001b[0m, in \u001b[0;36mload\u001b[0;34m(repo_or_dir, model, source, trust_repo, force_reload, verbose, skip_validation, *args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[39mif\u001b[39;00m source \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mgithub\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    563\u001b[0m     repo_or_dir \u001b[39m=\u001b[39m _get_cache_or_reload(repo_or_dir, force_reload, trust_repo, \u001b[39m\"\u001b[39m\u001b[39mload\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    564\u001b[0m                                        verbose\u001b[39m=\u001b[39mverbose, skip_validation\u001b[39m=\u001b[39mskip_validation)\n\u001b[0;32m--> 566\u001b[0m model \u001b[39m=\u001b[39m _load_local(repo_or_dir, model, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    567\u001b[0m \u001b[39mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/hub.py:595\u001b[0m, in \u001b[0;36m_load_local\u001b[0;34m(hubconf_dir, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    592\u001b[0m     hub_module \u001b[39m=\u001b[39m _import_module(MODULE_HUBCONF, hubconf_path)\n\u001b[1;32m    594\u001b[0m     entry \u001b[39m=\u001b[39m _load_entry_from_hubconf(hub_module, model)\n\u001b[0;32m--> 595\u001b[0m     model \u001b[39m=\u001b[39m entry(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    597\u001b[0m \u001b[39mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/quant_yolov5/./hubconf.py:89\u001b[0m, in \u001b[0;36mcustom\u001b[0;34m(path, autoshape, classes, cfg, _verbose, device)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcustom\u001b[39m(path\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpath/to/model.pt\u001b[39m\u001b[39m'\u001b[39m, autoshape\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,classes\u001b[39m=\u001b[39m\u001b[39m80\u001b[39m,cfg\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, _verbose\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, device\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m     88\u001b[0m     \u001b[39m# YOLOv5 custom or local model\u001b[39;00m\n\u001b[0;32m---> 89\u001b[0m     \u001b[39mreturn\u001b[39;00m _create(path, autoshape\u001b[39m=\u001b[39;49mautoshape, pretrained\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, classes \u001b[39m=\u001b[39;49m classes,cfg\u001b[39m=\u001b[39;49mcfg,verbose\u001b[39m=\u001b[39;49m_verbose, device\u001b[39m=\u001b[39;49mdevice)\n",
      "File \u001b[0;32m~/quant_yolov5/./hubconf.py:84\u001b[0m, in \u001b[0;36m_create\u001b[0;34m(name, pretrained, channels, classes, autoshape, verbose, device, cfg)\u001b[0m\n\u001b[1;32m     82\u001b[0m help_url \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mhttps://docs.ultralytics.com/yolov5/tutorials/pytorch_hub_model_loading\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     83\u001b[0m s \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m. Cache may be out of date, try `force_reload=True` or see \u001b[39m\u001b[39m{\u001b[39;00mhelp_url\u001b[39m}\u001b[39;00m\u001b[39m for help.\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m---> 84\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(s) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "\u001b[0;31mException\u001b[0m: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 804: forward compatibility was attempted on non supported HW. Cache may be out of date, try `force_reload=True` or see https://docs.ultralytics.com/yolov5/tutorials/pytorch_hub_model_loading for help."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "model_pre = torch.hub.load('.',\n",
    "                      'custom',\n",
    "                      'experiment_models/lpyolo.pt',\n",
    "                       source='local',\n",
    "                       classes = 7,\n",
    "                       cfg = \"models/lpyolo.yaml\",\n",
    "                       force_reload=True\n",
    "                      )\n",
    "\n",
    "im = '/home/omar/datasets/coco128/images/train2017/000000579736.jpg'  # file, Path, PIL.Image, OpenCV, nparray, list\n",
    "results = model_pre(im)  # inference\n",
    "results.show()  # or .show(), .save(), .crop(), .pandas(), etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Quantized Model (QAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GMusP4OAxFu6"
   },
   "outputs": [],
   "source": [
    "# YOLOv5 PyTorch HUB Inference (DetectionModels only)\n",
    "import torch\n",
    "\n",
    "model_q = torch.hub.load('.',\n",
    "                       'custom',\n",
    "                       'experiment_models/lpyolo_W4A4.pt',\n",
    "                       source='local',\n",
    "                       classes = 7,\n",
    "                       force_reload=True,\n",
    "                       cfg = \"models/lpyolo_quant.yaml\",\n",
    "                      )\n",
    "im = 'https://ultralytics.com/images/bus.jpg'\n",
    "#im = '/home/omar/datasets/coco128/images/train2017/000000000338.jpg'  # file, Path, PIL.Image, OpenCV, nparray, list\n",
    "results = model_q(im)  # inference\n",
    "results.show()  # or .show(), .save(), .crop(), .pandas(), etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from brevitas.export import export_brevitas_onnx\n",
    "import torch\n",
    "\n",
    "IN_CH = 384\n",
    "OUT_CH = 640\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "model_no_detect = torch.nn.Sequential(*[model_q.model.model.model[i] for i in range(15)])\n",
    "\n",
    "path = 'experiment_models/lpyolo_W4A4.onnx'\n",
    "inp = torch.randn(BATCH_SIZE,3, IN_CH, OUT_CH).cuda()\n",
    "\n",
    "detection_model = model_no_detect\n",
    "detection_model.cuda()\n",
    "detection_model.eval()\n",
    "\n",
    "exported_model = export_brevitas_onnx(detection_model, inp, path)\n",
    "\n",
    "detect_module = model_q.model.model.model[15]\n",
    "torch.save(detect_module.state_dict(), 'experiment_models/detect_module.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running model with detached detect head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from models.yolo import Detect\n",
    "from utils.general import non_max_suppression\n",
    "from utils.dataloaders import letterbox\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "nc = 7\n",
    "anchors = np.array([[10, 13, 16, 30, 33, 23], [81, 82, 135, 169, 344, 319], [116, 90, 156, 198, 373, 326]])\n",
    "detect_head = Detect(nc, anchors,ch=[36,36,36],do_quant=False,inplace=True)\n",
    "detect_head.load_state_dict(torch.load(\"experiment_models/detect_module.pt\"))\n",
    "detect_head.cuda()\n",
    "detect_head.eval()\n",
    "detect_head.training = False\n",
    "detect_head.stride = [32,32,32]\n",
    "\n",
    "\n",
    "image = cv2.imread(\"data/images/zidane.jpg\")\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "image = letterbox(image, (384,640), auto=False)[0]\n",
    "\n",
    "image = image.astype(np.uint8)\n",
    "image = np.expand_dims(image, 0)\n",
    "image = np.transpose(image, (0,3,1,2))\n",
    "image = torch.from_numpy(image).to(\"cuda\").float()\n",
    "\n",
    "model_no_detect.eval().cuda()\n",
    "model_no_detect_out = model_no_detect(image).value.detach().to(\"cuda\")\n",
    "\n",
    "pred = detect_head([model_no_detect_out,model_no_detect_out,model_no_detect_out])\n",
    "print(pred[0].shape)\n",
    "pred = pred[0].detach()\n",
    "\n",
    "detections = non_max_suppression(pred,conf_thres = 0.30, iou_thres=0.45)\n",
    "print(detections)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Detect seperated model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detect_head.m[0](model_no_detect_out).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_q.model.model.model[15].m[0].weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_no_detect[0].conv.out_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_q.model.model.model[15].conv.int_weight()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_q.model.model.model[15].m[0].quant_bias()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Coco Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def count_files_in_directory(directory_path):\n",
    "    # Get the list of files in the directory\n",
    "    files = os.listdir(directory_path)\n",
    "\n",
    "    # Count the number of files\n",
    "    file_count = len(files)\n",
    "\n",
    "    return file_count\n",
    "\n",
    "\n",
    "train_img_path = \"../datasets/coco128/images/train2017\"  \n",
    "train_label_path = \"../datasets/coco128/labels/train2017\" \n",
    "train_img_dir = os.listdir(train_img_path)\n",
    "train_label_dir = os.listdir(train_label_path)\n",
    "\n",
    "val_img_path = \"../datasets/coco128/images/val2017\"  \n",
    "val_label_path = \"../datasets/coco128/labels/val2017\" \n",
    "val_img_dir = os.listdir(val_img_path)\n",
    "val_label_dir = os.listdir(val_label_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img_len = count_files_in_directory(train_img_path)\n",
    "train_label_len = count_files_in_directory(train_label_path)\n",
    "\n",
    "val_img_len = count_files_in_directory(val_img_path)\n",
    "val_label_len = count_files_in_directory(val_label_path)\n",
    "\n",
    "print(train_img_len, train_label_len)\n",
    "print(val_img_len, val_label_len)\n",
    "\n",
    "train_img_files = [img_path.replace(\".jpg\",\".txt\") for img_path in train_img_dir]\n",
    "\n",
    "train_imgs_to_delete = list(set(train_img_files) ^ set(train_label_dir))\n",
    "train_imgs_to_delete = [img_path.replace(\".txt\",\".jpg\") for img_path in train_imgs_to_delete]\n",
    "\n",
    "for train_img_to_delete in train_imgs_to_delete:\n",
    "    path = train_img_path + \"/\" + train_img_to_delete\n",
    "    if os.path.exists(path):\n",
    "        os.remove(path)\n",
    "    \n",
    "print(len(os.listdir(train_img_path)))\n",
    "\n",
    "val_img_files = [img_path.replace(\".jpg\",\".txt\") for img_path in val_img_dir]\n",
    "\n",
    "val_imgs_to_delete = list(set(val_img_files) ^ set(val_label_dir))\n",
    "val_imgs_to_delete = [img_path.replace(\".txt\",\".jpg\") for img_path in val_imgs_to_delete]\n",
    "\n",
    "for val_img_to_delete in val_imgs_to_delete:\n",
    "    path = val_img_path + \"/\" + val_img_to_delete\n",
    "    if os.path.exists(path):\n",
    "        os.remove(path)\n",
    "    \n",
    "print(len(os.listdir(val_img_path)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "idx = 2\n",
    "\n",
    "image_path = val_img_dir[idx]\n",
    "print(image_path)\n",
    "print(train_img_path)\n",
    "\n",
    "img = cv2.imread(val_img_path + \"/\" + image_path)\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "img = cv2.resize(img,(380,640))\n",
    "\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_path = val_label_path + \"/000000177934.txt\"\n",
    "with open(label_path, 'r') as file:\n",
    "    # Read the content of the file\n",
    "    file_content = file.read()\n",
    "\n",
    "    # Print the content\n",
    "    print(file_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean VOC data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "directory_path = \"../datasets/VOC/labels/*\"\n",
    "label_files = glob.glob(f'{directory_path}/*.txt')\n",
    "\n",
    "\n",
    "count = 0\n",
    "for label_file in label_files:\n",
    "  with open(label_file, 'r') as file:\n",
    "    # Read the entire content of the file\n",
    "    file_content = file.read()\n",
    "    if not file_content:\n",
    "      count += 1\n",
    "      print(f\"label file {label_file} is empty\")\n",
    "\n",
    "      train_file = label_file.replace(\"/labels/\",\"/images/\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "YOLOv5 Tutorial",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
